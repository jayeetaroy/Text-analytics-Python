{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The twitter_samples corpus contains 3 files.\n",
    "\n",
    "1) negative_tweets.json: contains 5k negative tweets\n",
    "2) positive_tweets.json: contains 5k positive tweets\n",
    "3) tweets.20150430-223406.json: contains 20k positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "print (twitter_samples.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n",
      "20000\n",
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
      "@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
      "@97sides CONGRATS :)\n",
      "yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n"
     ]
    }
   ],
   "source": [
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "print (len(pos_tweets)) # Output: 5000\n",
    " \n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "print (len(neg_tweets)) # Output: 5000\n",
    " \n",
    "all_tweets = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "print (len(all_tweets)) # Output: 20000\n",
    " \n",
    "for tweet in pos_tweets[:5]:\n",
    "    print (tweet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three different parameters can be passed while calling the TweetTokenizer class. They are:\n",
    "\n",
    "preserve_case: if False then it converts tweet to lowercase and vice-versa.\n",
    "strip_handles: if True then it removes twitter handles from the tweet and vice-versa.\n",
    "reduce_len: if True then it reduces the length of words in the tweet like hurrayyyy, yipppiieeee, etc. and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#followfriday', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n",
      "['hey', 'james', '!', 'how', 'odd', ':/', 'please', 'call', 'our', 'contact', 'centre', 'on', '02392441234', 'and', 'we', 'will', 'be', 'able', 'to', 'assist', 'you', ':)', 'many', 'thanks', '!']\n",
      "['we', 'had', 'a', 'listen', 'last', 'night', ':)', 'as', 'you', 'bleed', 'is', 'an', 'amazing', 'track', '.', 'when', 'are', 'you', 'in', 'scotland', '?', '!']\n",
      "['congrats', ':)']\n",
      "['yeaaah', 'yipppy', '!', '!', '!', 'my', 'accnt', 'verified', 'rqst', 'has', 'succeed', 'got', 'a', 'blue', 'tick', 'mark', 'on', 'my', 'fb', 'profile', ':)', 'in', '15', 'days']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    " \n",
    "for tweet in pos_tweets[:5]:\n",
    "    print (tweet_tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#clean the tweets \n",
    "– Remove stock market tickers like $GE\n",
    "– Remove retweet text “RT”\n",
    "– Remove hyperlinks\n",
    "– Remove hashtags (only the hashtag # and not the word)\n",
    "– Remove stop words like a, and, the, is, are, etc.\n",
    "– Remove emoticons like :), :D, :(, :-), etc.\n",
    "– Remove punctuation like full-stop, comma, exclamation sign, etc.\n",
    "– Convert words to Stem/Base words using Porter Stemming Algorithm. E.g. words like ‘working’, ‘works’, and ‘worked’ will be converted to their base/stem word “work”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'great', 'day', 'good', 'morn']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    " \n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    " \n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "from nltk.tokenize import TweetTokenizer\n",
    " \n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    " \n",
    "def clean_tweets(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    " \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "              word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    " \n",
    "    return tweets_clean\n",
    " \n",
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    " \n",
    "# print cleaned tweet\n",
    "print (clean_tweets(custom_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@BhaktisBanter @PallaviRuhail This one is irresistible :)\n",
      "#FlipkartFashionFriday http://t.co/EbZ0L2VENM\n"
     ]
    }
   ],
   "source": [
    "print (pos_tweets[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'irresist', 'flipkartfashionfriday']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (clean_tweets(pos_tweets[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': True, 'great': True, 'day': True, 'good': True, 'morn': True}\n"
     ]
    }
   ],
   "source": [
    "# feature extractor function\n",
    "def bag_of_words(tweet):\n",
    "    words = clean_tweets(tweet)\n",
    "    words_dictionary = dict([word, True] for word in words)    \n",
    "    return words_dictionary\n",
    " \n",
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "print (bag_of_words(custom_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000\n"
     ]
    }
   ],
   "source": [
    "# positive tweets feature set\n",
    "pos_tweets_set = []\n",
    "for tweet in pos_tweets:\n",
    "    pos_tweets_set.append((bag_of_words(tweet), 'pos'))    \n",
    " \n",
    "# negative tweets feature set\n",
    "neg_tweets_set = []\n",
    "for tweet in neg_tweets:\n",
    "    neg_tweets_set.append((bag_of_words(tweet), 'neg'))\n",
    " \n",
    "print(len(pos_tweets_set), len(neg_tweets_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create test and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 8000\n"
     ]
    }
   ],
   "source": [
    "# radomize pos_reviews_set and neg_reviews_set\n",
    "# doing so will output different accuracy result everytime we run the program\n",
    "from random import shuffle \n",
    "shuffle(pos_tweets_set)\n",
    "shuffle(neg_tweets_set)\n",
    " \n",
    "test_set = pos_tweets_set[:1000] + neg_tweets_set[:1000]\n",
    "train_set = pos_tweets_set[1000:] + neg_tweets_set[1000:]\n",
    " \n",
    "print(len(test_set),  len(train_set)) # Output: (2000, 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.741\n",
      "Most Informative Features\n",
      "                     sad = True              neg : pos    =     39.0 : 1.0\n",
      "                   arriv = True              pos : neg    =     37.0 : 1.0\n",
      "                     bam = True              pos : neg    =     26.3 : 1.0\n",
      "                     x15 = True              neg : pos    =     17.0 : 1.0\n",
      "                    glad = True              pos : neg    =     15.8 : 1.0\n",
      "                 appreci = True              pos : neg    =     15.7 : 1.0\n",
      "                  commun = True              pos : neg    =     15.0 : 1.0\n",
      "                      aw = True              neg : pos    =     14.2 : 1.0\n",
      "                opportun = True              pos : neg    =     13.7 : 1.0\n",
      "                    miss = True              neg : pos    =     12.6 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    " \n",
    "accuracy = classify.accuracy(classifier, test_set)\n",
    "print(accuracy) # Output: 0.765\n",
    " \n",
    "print (classifier.show_most_informative_features(10))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n"
     ]
    }
   ],
   "source": [
    "#testing classifier with a custom tweet\n",
    "custom_tweet = \"I hated the film. It was a disaster. Poor direction, bad acting.\"\n",
    "custom_tweet_set = bag_of_words(custom_tweet)\n",
    "print (classifier.classify(custom_tweet_set))\n",
    "# Negative tweet correctly classified as negative\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ProbDist with 2 samples>\n",
      "neg\n",
      "0.8883737135505989\n",
      "0.11162628644939976\n"
     ]
    }
   ],
   "source": [
    "# probability result\n",
    "prob_result = classifier.prob_classify(custom_tweet_set)\n",
    "print (prob_result) # Output: \n",
    "print (prob_result.max()) \n",
    "print (prob_result.prob(\"neg\"))\n",
    "print (prob_result.prob(\"pos\")) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n",
      "<ProbDist with 2 samples>\n",
      "pos\n",
      "0.0006600524667181052\n",
      "0.9993399475332831\n"
     ]
    }
   ],
   "source": [
    " \n",
    "custom_tweet = \"It was a wonderful and amazing movie. I loved it. Best direction, good acting.\"\n",
    "custom_tweet_set = bag_of_words(custom_tweet)\n",
    " \n",
    "print (classifier.classify(custom_tweet_set)) # Output: pos\n",
    "# Positive tweet correctly classified as positive\n",
    " \n",
    "# probability result\n",
    "prob_result = classifier.prob_classify(custom_tweet_set)\n",
    "print (prob_result) \n",
    "print (prob_result.max()) \n",
    "print (prob_result.prob(\"neg\"))\n",
    "print (prob_result.prob(\"pos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the model accuracy, deciding the parameters\n",
    "#Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "#Precision = (TP) / (TP + FP)\n",
    "#Recall = (TP) / (TP + FN)\n",
    "#F1 Score = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    " \n",
    "actual_set = defaultdict(set)\n",
    "predicted_set = defaultdict(set)\n",
    " \n",
    "actual_set_cm = []\n",
    "predicted_set_cm = []\n",
    " \n",
    "for index, (feature, actual_label) in enumerate(test_set):\n",
    "    actual_set[actual_label].add(index)\n",
    "    actual_set_cm.append(actual_label)\n",
    " \n",
    "    predicted_label = classifier.classify(feature)\n",
    " \n",
    "    predicted_set[predicted_label].add(index)\n",
    "    predicted_set_cm.append(predicted_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos precision: 0.7372047244094488\n",
      "pos recall: 0.749\n",
      "pos F-measure: 0.7430555555555555\n",
      "neg precision: 0.7449186991869918\n",
      "neg recall: 0.733\n",
      "neg F-measure: 0.7389112903225806\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import precision, recall, f_measure, ConfusionMatrix\n",
    " \n",
    "print ('pos precision:', precision(actual_set['pos'], predicted_set['pos'])) # Output: pos precision: 0.762896825397\n",
    "print ('pos recall:', recall(actual_set['pos'], predicted_set['pos'])) # Output: pos recall: 0.769\n",
    "print ('pos F-measure:', f_measure(actual_set['pos'], predicted_set['pos']) )# Output: pos F-measure: 0.76593625498\n",
    " \n",
    "print ('neg precision:', precision(actual_set['neg'], predicted_set['neg'])) # Output: neg precision: 0.767137096774\n",
    "print ('neg recall:', recall(actual_set['neg'], predicted_set['neg'])) # Output: neg recall: 0.761\n",
    "print ('neg F-measure:', f_measure(actual_set['neg'], predicted_set['neg'])) # Output: neg F-measure: 0.7640562249"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n           |   Predicted NO      |   Predicted YES     |\\n-----------+---------------------+---------------------+\\nActual NO  | True Negative (TN)  | False Positive (FP) |\\nActual YES | False Negative (FN) | True Positive (TP)  |\\n-----------+---------------------+---------------------+\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making the confusion matrix\n",
    "\n",
    "'''\n",
    "           |   Predicted NO      |   Predicted YES     |\n",
    "-----------+---------------------+---------------------+\n",
    "Actual NO  | True Negative (TN)  | False Positive (FP) |\n",
    "Actual YES | False Negative (FN) | True Positive (TP)  |\n",
    "-----------+---------------------+---------------------+\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   n   p |\n",
      "    |   e   o |\n",
      "    |   g   s |\n",
      "----+---------+\n",
      "neg |<733>267 |\n",
      "pos | 251<749>|\n",
      "----+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = ConfusionMatrix(actual_set_cm, predicted_set_cm)\n",
    "print (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |      n      p |\n",
      "    |      e      o |\n",
      "    |      g      s |\n",
      "----+---------------+\n",
      "neg | <36.6%> 13.3% |\n",
      "pos |  12.6% <37.5%>|\n",
      "----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output can be interpreted as:\n",
    "– 761 negative tweets were correctly classified as negative (TN)\n",
    "– 239 negative tweets were incorrectly classified as positive (FP)\n",
    "– 231 positive tweets were incorrectly classified as negative (FN)\n",
    "– 769 positive tweets were correctly classified as positive (TP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
